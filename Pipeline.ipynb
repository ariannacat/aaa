{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata as an\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from typing import Optional, Union, Dict\n",
    "from scanpy.get import _get_obs_rep, _set_obs_rep\n",
    "from scipy.sparse import issparse\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preliminary_functions as pr\n",
    "import data_utils as du\n",
    "import classifier as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"unfiltered_data_1.txt\",\n",
    "                                engine = 'python', index_col = 0)\n",
    "df_2 = pd.read_csv(\"unfiltered_data_2.txt\",\n",
    "                                engine = 'python', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBSAMPLE & MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_1.shape[0] > df_2.shape[0]:\n",
    "    df_1_sample = df_1.sample(df_2.shape[0], axis = 0)\n",
    "    df = pd.concat([df_1_sample, df_2])\n",
    "elif df_2.shape[0] > df_1.shape[0]:\n",
    "    df_2_sample = df_2.sample(df_1.shape[0], axis = 0) \n",
    "    df = pd.concat([df_1, df_2_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Dataframe 1': df_1, 'Dataframe 2': df_2, 'Dataframe Merged': df}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA INSPECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = {}\n",
    "for name, dataframe in data.items():\n",
    "    print(f'=== PROCESSING DATAFRAME {dataframe} ===')\n",
    "    print(f'Shape before filtering: {dataframe.shape}')\n",
    "\n",
    "    pr.remove_noncoding(dataframe) #Removing non-coding genes\n",
    "    print(f'Shape after removing non-coding genes: {dataframe.shape}')\n",
    "\n",
    "    dataframe.dropna(axis=1, inplace=True) #Removing missing values\n",
    "    print(f'Shape after removing missing values: {dataframe.shape}')\n",
    "\n",
    "    dataframe = pr.remove_duplicates(dataframe) #Removing duplicates\n",
    "    print(f'Shape after removing duplicates: {dataframe.shape}')\n",
    "\n",
    "    dataframe = pr.sparsity_threshold(dataframe) #Removing columns with sparsity exceeding the threshold\n",
    "    print(f'Shape after removing duplicates: {dataframe.shape}')\n",
    "\n",
    "    adata = an.AnnData(dataframe.iloc[:,2:].astype(int)) #Converting dataframe into AnnData object\n",
    "    adata.obs[\"Condition\"] = dataframe[\"Condition\"]\n",
    "    adata = pr.remove_mito(adata) #Removing cells with high mitochondrial count\n",
    "\n",
    "    processed_dataframe = pd.DataFrame(adata.X)\n",
    "    processed_dataframe.index = adata.obs_names\n",
    "    processed_dataframe.columns = adata.var_names\n",
    "    processed_dataframe.insert(0, 'Condition', adata.obs[\"Condition\"])\n",
    "    \n",
    "    processed_data[name] = processed_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANING \n",
    "This step is specific to the dataset that I used, please check `data_utils.py` for further information about this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_filtered = {}\n",
    "\n",
    "for name, df_filtered in processed_data.items():\n",
    "    df_clean = du.data_cleaning(df_filtered)\n",
    "\n",
    "    clean_data_filtered[name] = df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_raw = {}\n",
    "for name,d in data:\n",
    "    df_clean = du.data_cleaning(d)\n",
    "\n",
    "    clean_data_raw[name] = df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clean_data_raw['Dataframe 1'].shape[0] > clean_data_raw['Dataframe 2'].shape[0]:\n",
    "    clean_data_raw['Dataframe 1'] = clean_data_raw['Dataframe 1'].sample(clean_data_raw['Dataframe 2'].shape[0], axis = 0)\n",
    "elif clean_data_raw['Dataframe 2'].shape[0] > clean_data_raw['Dataframe 1'].shape[0]:\n",
    "    clean_data_raw['Dataframe 2'] = clean_data_raw['Dataframe 2'].sample(clean_data_raw['Dataframe 1'].shape[0], axis = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []  # List to collect results\n",
    "\n",
    "for name, df in clean_data_filtered.items():\n",
    "    print(f'=== PROCESSING DATAFRAME {name} FILTERED ===')\n",
    "    X = df.iloc[:, 2:]\n",
    "    y = df[\"Condition\"]\n",
    "\n",
    "    max_accuracy, avg_accuracy, avg_mse, avg_roc = cl.tune_sdg_classifier(X, y)\n",
    "\n",
    "    # Append results as a dictionary\n",
    "    results.append({\n",
    "        \"Dataset\": name,\n",
    "        \"Max Accuracy\": max_accuracy,\n",
    "        \"Avg Accuracy\": avg_accuracy,\n",
    "        \"Avg MSE\": avg_mse,\n",
    "        \"Avg ROC AUC\": avg_roc\n",
    "    })\n",
    "\n",
    "# Convert the list of results into a DataFrame\n",
    "filtered_enet_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_results = []  # List to store the results\n",
    "\n",
    "for name, df in clean_data_raw.items():\n",
    "    print(f'=== PROCESSING DATAFRAME {name} RAW ===')\n",
    "\n",
    "    print(f'=== BINNING ===')\n",
    "    adata = an.AnnData(df.iloc[:,1:].astype(int))\n",
    "    adata.obs[\"Condition\"] = df[\"Condition\"]\n",
    "\n",
    "    bins = cl.tune_binning_enet(adata)\n",
    "\n",
    "    preprocessor = cl.Preprocessor(\n",
    "        use_key = \"X\",\n",
    "        filter_gene_by_counts=0,\n",
    "        filter_cell_by_counts=False,\n",
    "        normalize_total=False,\n",
    "        result_normed_key=\"X_normed\",\n",
    "        log1p=False,\n",
    "        result_log1p_key=\"X_log1p\",\n",
    "        subset_hvg=False,\n",
    "        hvg_flavor=\"seurat_v3\",\n",
    "        binning=bins,\n",
    "        result_binned_key=\"X_binned\"\n",
    "    )\n",
    "\n",
    "    preprocessor.__call__(adata)\n",
    "\n",
    "    df_enet = pd.DataFrame(adata.layers[\"X_binned\"])\n",
    "    df_enet.index = adata.obs_names\n",
    "    df_enet.columns = adata.var_names\n",
    "    df_enet.insert(0, 'Condition', adata.obs[\"Condition\"])\n",
    "\n",
    "    print('=== CLASSIFIER ===')\n",
    "\n",
    "    X = df_enet.iloc[:, 1:]\n",
    "    y = df_enet[\"Condition\"]\n",
    "\n",
    "    max_accuracy, avg_accuracy, avg_mse, avg_roc = cl.tune_sdg_classifier(X, y)\n",
    "\n",
    "    # Save results\n",
    "    enet_results.append({\n",
    "        \"Dataset\": name,\n",
    "        \"Max Accuracy\": max_accuracy,\n",
    "        \"Avg Accuracy\": avg_accuracy,\n",
    "        \"Avg MSE\": avg_mse,\n",
    "        \"Avg ROC AUC\": avg_roc\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "raw_enet_results = pd.DataFrame(enet_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []  # List to collect results\n",
    "\n",
    "for name, df in clean_data_filtered.items():\n",
    "    print(f'=== PROCESSING DATAFRAME {name} FILTERED ===')\n",
    "    X = df.iloc[:, 2:]\n",
    "    y = df[\"Condition\"]\n",
    "\n",
    "    max_accuracy, avg_accuracy, avg_mse, avg_roc = cl.tune_xgb_classifier(X, y)\n",
    "\n",
    "    # Append results as a dictionary\n",
    "    results.append({\n",
    "        \"Dataset\": name,\n",
    "        \"Max Accuracy\": max_accuracy,\n",
    "        \"Avg Accuracy\": avg_accuracy,\n",
    "        \"Avg MSE\": avg_mse,\n",
    "        \"Avg ROC AUC\": avg_roc\n",
    "    })\n",
    "\n",
    "# Convert the list of results into a DataFrame\n",
    "filtered_xgb_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results = []  # List to store the results\n",
    "\n",
    "for name, df in clean_data_raw.items():\n",
    "    print(f'=== PROCESSING DATAFRAME {name} RAW ===')\n",
    "\n",
    "    print(f'=== BINNING ===')\n",
    "    adata = an.AnnData(df.iloc[:,1:].astype(int))\n",
    "    adata.obs[\"Condition\"] = df[\"Condition\"]\n",
    "\n",
    "    bins = cl.tune_binning_xgb(adata)\n",
    "\n",
    "    preprocessor = cl.Preprocessor(\n",
    "        use_key = \"X\",\n",
    "        filter_gene_by_counts=0,\n",
    "        filter_cell_by_counts=False,\n",
    "        normalize_total=False,\n",
    "        result_normed_key=\"X_normed\",\n",
    "        log1p=False,\n",
    "        result_log1p_key=\"X_log1p\",\n",
    "        subset_hvg=False,\n",
    "        hvg_flavor=\"seurat_v3\",\n",
    "        binning=bins,\n",
    "        result_binned_key=\"X_binned\"\n",
    "    )\n",
    "\n",
    "    preprocessor.__call__(adata)\n",
    "\n",
    "    df_xgb = pd.DataFrame(adata.layers[\"X_binned\"])\n",
    "    df_xgb.index = adata.obs_names\n",
    "    df_xgb.columns = adata.var_names\n",
    "    df_xgb.insert(0, 'Condition', adata.obs[\"Condition\"])\n",
    "\n",
    "    print('=== CLASSIFIER ===')\n",
    "\n",
    "    X = df_xgb.iloc[:, 1:]\n",
    "    y = df_xgb[\"Condition\"]\n",
    "\n",
    "    max_accuracy, avg_accuracy, avg_mse, avg_roc = cl.tune_xgb_classifier(X, y)\n",
    "\n",
    "    # Save results\n",
    "    xgb_results.append({\n",
    "        \"Dataset\": name,\n",
    "        \"Max Accuracy\": max_accuracy,\n",
    "        \"Avg Accuracy\": avg_accuracy,\n",
    "        \"Avg MSE\": avg_mse,\n",
    "        \"Avg ROC AUC\": avg_roc\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "raw_xgb_results = pd.DataFrame(xgb_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
